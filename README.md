# Notice

This project is in early development, the Docker files are not working as of now and a lot is still missing. Not sure how much broke after the cleanup for upload. This also only works for the Mistral chat template, I'm only supporting what I use myself.

# MultiModelProxy

An OAI-compatible proxy server that facilitates fast Chain of Thought generation thought prompting by using a smaller model to do the CoT inference before handing it to the (larger) main model.

# License

This project is licensed under AGPLv3.0 (see included LICENSE file). The following clause applies on top of it and overrides any conflicting clauses:

**This project may not be used in a commercial context under any circumstance unless a commercial license has been granted by the owner. This stipulation applies on top of the
AGPLv3 license.**
